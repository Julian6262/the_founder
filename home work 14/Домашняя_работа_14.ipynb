{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNN1qJmGolxbN/Sb62MYkai",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Julian6262/the_founder/blob/main/home%20work%2014/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai mwclient mwparserfromhell tiktoken aiogram"
      ],
      "metadata": {
        "id": "2md2coGxFz5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "MkHgYJDxF1MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mwclient, mwparserfromhell\n",
        "import openai, tiktoken\n",
        "import os, getpass\n",
        "import pandas as pd\n",
        "import re\n",
        "from scipy import spatial\n",
        "import ast"
      ],
      "metadata": {
        "id": "T0E6DPGIF2UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORY_TITLE = \"Category:Russian chemists\"\n",
        "WIKI_SITE = \"en.wikipedia.org\"\n",
        "\n",
        "GPT_MODEL = \"gpt-3.5-turbo\"\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "\n",
        "SAVE_PATH = \"./Russian_chemists.csv\""
      ],
      "metadata": {
        "id": "mZav-GBcF5N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Введите OpenAI API Key:\")\n",
        "client = openai.OpenAI(api_key = os.environ.get(\"OPENAI_API_KEY\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKfnwzvBF60Q",
        "outputId": "068a7196-377e-4e3b-a5aa-83ce4fa99d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Введите OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Создаем базу знаний по запросу \"Russian_chemists\"**"
      ],
      "metadata": {
        "id": "IoHB62Cd9KP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Сылка на готовый файл базы знаний:**  https://drive.google.com/file/d/1A_q0v0mu8JI4YZYUbVlb2qcmSfGqxL6i/view?usp=sharing"
      ],
      "metadata": {
        "id": "nxHhJ5ZT9nZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Соберем заголовки всех статей\n",
        "def titles_from_category(category: mwclient.listing.Category, max_depth: int) -> set[str]:\n",
        "    \"\"\"Возвращает набор заголовков страниц в данной категории Википедии и ее подкатегориях.\"\"\"\n",
        "    titles = set() # Используем множество для хранения заголовков статей\n",
        "    for cm in category.members(): # Перебираем вложенные объекты категории\n",
        "        if type(cm) == mwclient.page.Page: # Если объект является страницей\n",
        "            titles.add(cm.name) # в хранилище заголовков добавляем имя страницы\n",
        "        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0: # Если объект является категорией и глубина вложения не достигла максимальной\n",
        "            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1) # вызываем рекурсивно функцию для подкатегории\n",
        "            titles.update(deeper_titles) # добавление в множество элементов из другого множества\n",
        "    return titles\n",
        "\n",
        "site = mwclient.Site(WIKI_SITE)\n",
        "\n",
        "# Загрузка раздела заданной категории\n",
        "category_page = site.pages[CATEGORY_TITLE]\n",
        "# Получение множества всех заголовков категории с вложенностью на один уровень\n",
        "titles = titles_from_category(category_page, max_depth=1)\n",
        "\n",
        "\n",
        "print(f\"Создано {len(titles)} заголовков статей в категории {CATEGORY_TITLE}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q-CaVg3qwp3",
        "outputId": "1057a4b8-ed76-445d-bcba-a1bf0d833645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Создано 192 заголовков статей в категории Category:Russian_chemists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Задаем секции, которые будут отброшены при парсинге статей\n",
        "SECTIONS_TO_IGNORE = [\n",
        "    \"See also\",\n",
        "    \"References\",\n",
        "    \"External links\",\n",
        "    \"Further reading\",\n",
        "    \"Footnotes\",\n",
        "    \"Bibliography\",\n",
        "    \"Sources\",\n",
        "    \"Citations\",\n",
        "    \"Literature\",\n",
        "    \"Footnotes\",\n",
        "    \"Notes and references\",\n",
        "    \"Photo gallery\",\n",
        "    \"Works cited\",\n",
        "    \"Photos\",\n",
        "    \"Gallery\",\n",
        "    \"Notes\",\n",
        "    \"References and sources\",\n",
        "    \"References and notes\",\n",
        "]"
      ],
      "metadata": {
        "id": "a5_7sYbvCr0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция возвращает список всех вложенных секций для заданной секции страницы Википедии\n",
        "def all_subsections_from_section(section: mwparserfromhell.wikicode.Wikicode, parent_titles: list[str], sections_to_ignore: set[str],) -> list[tuple[list[str], str]]:\n",
        "    # Извлекаем заголовки текущей секции\n",
        "    headings = [str(h) for h in section.filter_headings()]\n",
        "    title = headings[0]\n",
        "    # Заголовки Википедии имеют вид: \"== Heading ==\"\n",
        "\n",
        "    if title.strip(\"=\" + \" \") in sections_to_ignore:\n",
        "        # Если заголовок секции в списке для игнора, то пропускаем его\n",
        "        return []\n",
        "\n",
        "    # Объединим заголовки и подзаголовки, чтобы сохранить контекст для chatGPT\n",
        "    titles = parent_titles + [title]\n",
        "\n",
        "    # Преобразуем wikicode секции в строку\n",
        "    full_text = str(section)\n",
        "\n",
        "    # Выделяем текст секции без заголовка\n",
        "    section_text = full_text.split(title)[1]\n",
        "    if len(headings) == 1:\n",
        "        # Если один заголовок, то формируем результирующий список\n",
        "        return [(titles, section_text)]\n",
        "    else:\n",
        "        first_subtitle = headings[1]\n",
        "        section_text = section_text.split(first_subtitle)[0]\n",
        "        # Формируем результирующий список из текста до первого подзаголовка\n",
        "        results = [(titles, section_text)]\n",
        "        for subsection in section.get_sections(levels=[len(titles) + 1]):\n",
        "            results.extend(\n",
        "                # Вызываем функцию получения вложенных секций для заданной секции\n",
        "                all_subsections_from_section(subsection, titles, sections_to_ignore)\n",
        "                )  # Объединяем результирующие списки данной функции и вызываемой\n",
        "        return results\n",
        "\n",
        "\n",
        "# Функция возвращает список всех секций страницы, за исключением тех, которые отбрасываем\n",
        "def all_subsections_from_title(title: str, sections_to_ignore: set[str] = SECTIONS_TO_IGNORE, site_name: str = WIKI_SITE,) -> list[tuple[list[str], str]]:\n",
        "\n",
        "    site = mwclient.Site(site_name)\n",
        "\n",
        "    # Запрашиваем страницу по заголовку\n",
        "    page = site.pages[title]\n",
        "\n",
        "    # Получаем текстовое представление страницы\n",
        "    text = page.text()\n",
        "\n",
        "    # Удобный парсер для MediaWiki\n",
        "    parsed_text = mwparserfromhell.parse(text)\n",
        "    # Извлекаем заголовки\n",
        "    headings = [str(h) for h in parsed_text.filter_headings()]\n",
        "    if headings: # Если заголовки найдены\n",
        "        # В качестве резюме берем текст до первого заголовка\n",
        "        summary_text = str(parsed_text).split(headings[0])[0]\n",
        "    else:\n",
        "        # Если нет заголовков, то весь текст считаем резюме\n",
        "        summary_text = str(parsed_text)\n",
        "    results = [([title], summary_text)] # Добавляем резюме в результирующий список\n",
        "    for subsection in parsed_text.get_sections(levels=[2]): # Извлекаем секции 2-го уровня\n",
        "        results.extend(\n",
        "            # Вызываем функцию получения вложенных секций для заданной секции\n",
        "            all_subsections_from_section(subsection, [title], sections_to_ignore)\n",
        "        ) # Объединяем результирующие списки данной функции и вызываемой\n",
        "    return results"
      ],
      "metadata": {
        "id": "7BIJaIYqCu9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wikipedia_sections = []\n",
        "for title in titles:\n",
        "    wikipedia_sections.extend(all_subsections_from_title(title))\n",
        "print(f\"Найдено {len(wikipedia_sections)} секций на {len(titles)} страницах\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeafX96pC8kB",
        "outputId": "12bb8743-caab-4d41-c7f5-88aeac6c3bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Найдено 771 секций на 192 страницах\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Очистка текста секции от ссылок <ref>xyz</ref>, начальных и конечных пробелов\n",
        "def clean_section(section: tuple[list[str], str]) -> tuple[list[str], str]:\n",
        "    titles, text = section\n",
        "    # Удаляем ссылки\n",
        "    text = re.sub(r\"<ref.*?</ref>\", \"\", text)\n",
        "    # Удаляем пробелы вначале и конце\n",
        "    text = text.strip()\n",
        "    return (titles, text)\n",
        "\n",
        "\n",
        "def keep_section(section: tuple[list[str], str]) -> bool:\n",
        "    \"\"\"Возвращает значение True, если раздел должен быть сохранен, в противном случае значение False.\"\"\"\n",
        "    titles, text = section\n",
        "    # Фильтруем по произвольной длине, можно выбрать и другое значение\n",
        "    return False if len(text) < 16 else True\n",
        "\n",
        "\n",
        "wikipedia_sections = [clean_section(ws) for ws in wikipedia_sections]\n",
        "original_num_sections = len(wikipedia_sections)\n",
        "wikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]\n",
        "print(f\"Отфильтровано {original_num_sections-len(wikipedia_sections)} секций, осталось {len(wikipedia_sections)} секций.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKpauhgrCeO0",
        "outputId": "a93f56cc-e3a1-4bcf-d724-5b69357f8dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Отфильтровано 23 секций, осталось 748 секций.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция подсчета токенов\n",
        "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
        "    \"\"\"Возвращает число токенов в строке.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "\n",
        "# Функция разделения строк\n",
        "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
        "    \"\"\"Разделяет строку надвое с помощью разделителя (delimiter), пытаясь сбалансировать токены с каждой стороны.\"\"\"\n",
        "\n",
        "    # Делим строку на части по разделителю, по умолчанию \\n - перенос строки\n",
        "    chunks = string.split(delimiter)\n",
        "    if len(chunks) == 1:\n",
        "        return [string, \"\"]  # разделитель не найден\n",
        "    elif len(chunks) == 2:\n",
        "        return chunks  # нет необходимости искать промежуточную точку\n",
        "    else:\n",
        "        # Считаем токены\n",
        "        total_tokens = num_tokens(string)\n",
        "        halfway = total_tokens // 2\n",
        "        # Предварительное разделение по середине числа токенов\n",
        "        best_diff = halfway\n",
        "        # В цикле ищем какой из разделителей, будет ближе всего к best_diff\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            left = delimiter.join(chunks[: i + 1])\n",
        "            left_tokens = num_tokens(left)\n",
        "            diff = abs(halfway - left_tokens)\n",
        "            if diff >= best_diff:\n",
        "                break\n",
        "            else:\n",
        "                best_diff = diff\n",
        "        left = delimiter.join(chunks[:i])\n",
        "        right = delimiter.join(chunks[i:])\n",
        "        # Возвращаем левую и правую часть оптимально разделенной строки\n",
        "        return [left, right]\n",
        "\n",
        "\n",
        "# Функция обрезает строку до максимально разрешенного числа токенов\n",
        "def truncated_string(string: str, model: str, max_tokens: int, print_warning: bool = True,) -> str:\n",
        "    \"\"\"Обрезка строки до максимально разрешенного числа токенов.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    encoded_string = encoding.encode(string)\n",
        "    # Обрезаем строку и декодируем обратно\n",
        "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
        "    if print_warning and len(encoded_string) > max_tokens:\n",
        "        print(f\"Предупреждение: Строка обрезана с {len(encoded_string)} токенов до {max_tokens} токенов.\")\n",
        "    # Усеченная строка\n",
        "    return truncated_string\n",
        "\n",
        "\n",
        "# Функция делит секции статьи на части по максимальному числу токенов\n",
        "def split_strings_from_subsection(subsection: tuple[list[str], str], max_tokens: int = 1000, model: str = GPT_MODEL, max_recursion: int = 5,) -> list[str]:\n",
        "    \"\"\"\n",
        "    Разделяет секции на список из частей секций, в каждой части не более max_tokens.\n",
        "    Каждая часть представляет собой кортеж родительских заголовков [H1, H2, ...] и текста (str).\n",
        "    \"\"\"\n",
        "    titles, text = subsection\n",
        "    string = \"\\n\\n\".join(titles + [text])\n",
        "    num_tokens_in_string = num_tokens(string)\n",
        "    # Если длина соответствует допустимой, то вернет строку\n",
        "    if num_tokens_in_string <= max_tokens:\n",
        "        return [string]\n",
        "    # если в результате рекурсия не удалось разделить строку, то просто усечем ее по числу токенов\n",
        "    elif max_recursion == 0:\n",
        "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
        "    # иначе разделим пополам и выполним рекурсию\n",
        "    else:\n",
        "        titles, text = subsection\n",
        "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]: # Пробуем использовать разделители от большего к меньшему (разрыв, абзац, точка)\n",
        "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
        "            if left == \"\" or right == \"\":\n",
        "                # если какая-либо половина пуста, повторяем попытку с более простым разделителем\n",
        "                continue\n",
        "            else:\n",
        "                # применим рекурсию на каждой половине\n",
        "                results = []\n",
        "                for half in [left, right]:\n",
        "                    half_subsection = (titles, half)\n",
        "                    half_strings = split_strings_from_subsection(\n",
        "                        half_subsection,\n",
        "                        max_tokens=max_tokens,\n",
        "                        model=model,\n",
        "                        max_recursion=max_recursion - 1, # уменьшаем максимальное число рекурсий\n",
        "                    )\n",
        "                    results.extend(half_strings)\n",
        "                return results\n",
        "    # иначе никакого разделения найдено не было, поэтому просто обрезаем строку (должно быть очень редко)\n",
        "    return [truncated_string(string, model=model, max_tokens=max_tokens)]"
      ],
      "metadata": {
        "id": "PI3oIWalMwWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Делим секции на части\n",
        "MAX_TOKENS = 1600\n",
        "wikipedia_strings = []\n",
        "for section in wikipedia_sections:\n",
        "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
        "\n",
        "print(f\"{len(wikipedia_sections)} секций Википедии поделены на {len(wikipedia_strings)} строк.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WB_OYg2NAYR",
        "outputId": "3c2b54d4-ffd7-4aef-f6f8-109d7f70bb9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "748 секций Википедии поделены на 775 строк.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- Код ниже не выполняем, есть готовый файл --"
      ],
      "metadata": {
        "id": "F6izWfcMOdYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция отправки chatGPT строки для ее токенизации (вычисления эмбедингов)\n",
        "# def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "#    return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
        "\n",
        "\n",
        "# df = pd.DataFrame({\"text\": wikipedia_strings})\n",
        "\n",
        "# df['embedding'] = df.text.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))"
      ],
      "metadata": {
        "id": "Vjpx1EqPOaOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.head()"
      ],
      "metadata": {
        "id": "X7LQbwXZ3gxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохранение результата\n",
        "# df.to_csv(SAVE_PATH, index=False)"
      ],
      "metadata": {
        "id": "rjcGquASEOO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Метод дообучения chatGPT Search-Ask**\n"
      ],
      "metadata": {
        "id": "gfJg6E1ZPMFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(SAVE_PATH)\n",
        "df['embedding'] = df['embedding'].apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "bMkKAyivP2Fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def strings_ranked_by_relatedness(query: str, df: pd.DataFrame, relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y), top_n: int = 100) -> tuple[list[str], list[float]]:\n",
        "    \"\"\"Возвращает строки и схожести, отсортированные от большего к меньшему\"\"\"\n",
        "    # Отправляем в OpenAI API пользовательский запрос для токенизации\n",
        "    query_embedding_response = openai.embeddings.create(model=EMBEDDING_MODEL,input=query,)\n",
        "    # Получен токенизированный пользовательский запрос\n",
        "    query_embedding = query_embedding_response.data[0].embedding\n",
        "    # Сравниваем пользовательский запрос с каждой токенизированной строкой DataFrame\n",
        "    strings_and_relatednesses = [(row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"])) for i, row in df.iterrows()]\n",
        "    # Сортируем по убыванию схожести полученный список\n",
        "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
        "    # Преобразовываем наш список в кортеж из списков\n",
        "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
        "    # Возвращаем n лучших результатов\n",
        "    return strings[:top_n], relatednesses[:top_n]"
      ],
      "metadata": {
        "id": "YnuOtfqKPPWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка на релевантность по запросу\n",
        "strings, relatednesses = strings_ranked_by_relatedness(\"Olga Bogdanova (chemist)\", df, top_n=5)\n",
        "for string, relatedness in zip(strings, relatednesses):\n",
        "    print(f\"{relatedness=:.3f}\")\n",
        "    display(string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "T3yIe5hyPY-n",
        "outputId": "deeca229-407d-4a07-817d-a3088d6a922e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relatedness=0.941\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Olga Bogdanova (chemist)\\n\\n== Biography ==\\n\\nIn the 1920s Bogdanova worked in a laboratory at a [[synthetic rubber]] factory.\\n\\nFrom the early 1930s she worked at the [http://www.ioc.ac.ru/english N. D. Zelinsky Institute of Organic Chemistry] of the [[Academy of Sciences of the Soviet Union]] (now: [[Russian Academy of Sciences]]). Bordanova was a student and, for many years, a colleague of [[Academician]]s [[Nikolay Zelinsky|N. D. Zelinsky]] and [[Aleksei Balandin|A. A. Balandin]].'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relatedness=0.935\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"Olga Bogdanova (chemist)\\n\\n{{short description|Soviet chemist}}\\n\\n{{Infobox scientist\\n| name              = Olga Bogdanova\\n| image             = Bogdanova O.K.jpg\\n| birth_place       = [[Mogilev]], [[Russian Empire]]\\n| caption           = Bogdanova in 1950\\n| birth_name        = Olga Konstantinovna Bogdanova\\n| birth_date        = 29 June n.s. (11 July o.s.) 1896\\n| death_date        = March 1982 (aged 85)\\n| citizenship       = [[USSR]]\\n| workplaces        = [[Academy of Sciences of the Soviet Union]], Institute of Organic Chemistry\\n| awards            = [[Order of the Red Banner of Labour]] 1953, 1967;\\nMedal “For Valorous Labour during the Great Patriotic War, 1941-1945”;\\t\\n<br>Medal “In Honor of the 800th Anniversary of the Founding of Moscow”;\\n[[USSR State Prize|Stalin Prize]] 1950\\n}}\\n\\n'''Olga Konstantinovna Bogdanova''' ([[Russian language|Russian]]: Ольга Константиновна Богда́нова; 29 June [[Old Style and New Style dates|n.s.]], 1896 — March 1982)\\xa0was a [[Soviet Union|Soviet]] [[chemist]] who specialized in organic catalysis.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relatedness=0.933\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Olga Bogdanova (chemist)\\n\\n== Awards and prizes ==\\n\\n* [[USSR State Prize|Stalin Prize]], 2nd degree (1950) – for the development and industrial application of a catalyst used in a new chemical process\\n* [[Order of the Red Banner of Labour]] (1953, 1967)\\n* Medals<ref name=\":0\" />'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relatedness=0.906\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Olga Bogdanova (chemist)\\n\\n== Biography ==\\n\\n=== Main achievements ===\\n\\n* In 1941—1942 A. A. Balandin, O. K. Bogdanova, and A. P. Shcheglova developed and implemented at a synthetic rubber factory a method for producing a gas-resistant (polysulfide or thio) rubber, which was widely used in the production of a “self-tightening” – or, more accurately, “self-sealing” when struck by bullets – coating for aircraft fuel tanks.\\n* In 1946—1952, a new method was developed for obtaining 1,3 butadiene from petroleum feedstock on chromium oxide catalysts, which found industrial application at synthetic rubber factories in Sterlitamak and Sumgait (O. K. Bogdanova and\\xa0A. P. Shcheglova)\\n* In 1974—1981 O. K. Bogdanova and D. P. Belomestnykh developed a way to produce styrene (vinylbenzene) and its homologues through the oxidative decomposition of alkyl-benzene over a complex chromium oxide catalyst, which surpassed all known industrial catalysts used to decompose ethylbenzene.\\n\\nBogdanova held a [[Doctor of Philosophy|Ph.D]]. in [[Chemistry]], and she was a senior [[research]] associate.\\n\\nShe was buried in the [[Vagankovo Cemetery]].<ref name=\":0\" />'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relatedness=0.891\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Olga Dontsova\\n\\n==Education and work==\\n\\nIn 1991 she defended her Ph.D. from the Chemistry Department of [[Moscow State University]] and in 1997 her doctoral dissertation \\'\\'development of chemical methods for studying the structure and function of complex ribonucleoprotein systems\\'\\' was published. Since 1999 she is the professor of Bio-organic chemistry, Department of Chemistry of Natural Compounds at [[MSU Faculty of Chemistry|Faculty of Chemistry]], [[Moscow State University]].<ref name=\"b\"/>\\n\\nShe is the member of the [[Russian Foundation for Basic Research|Russian Foundation for Basic Research Board]].'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name_category = CATEGORY_TITLE.split(\":\")[1]\n",
        "\n",
        "\n",
        "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
        "    \"\"\"Возвращает число токенов в строке для заданной модели\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "# Функция формирования запроса к chatGPT по пользовательскому вопросу и базе знаний\n",
        "def query_message(query: str, df: pd.DataFrame, model: str, token_budget: int) -> str:\n",
        "    \"\"\"Возвращает сообщение для GPT с соответствующими исходными текстами, извлеченными из фрейма данных (базы знаний).\"\"\"\n",
        "    strings, relatednesses = strings_ranked_by_relatedness(query, df) # получаем строки из базы знаний и соответствующую релевантность в сравнении этой строки и заданного вопроса\n",
        "    # Шаблон инструкции для chatGPT\n",
        "    message = f'Use the below articles on the {name_category} to answer the subsequent question. If the answer cannot be found in the articles, write \"I could not find an answer.\"'\n",
        "    # Шаблон для вопроса\n",
        "    question = f\"\\n\\nQuestion: {query}\"\n",
        "\n",
        "    # Добавляем к сообщению для chatGPT релевантные строки из базы знаний, пока не выйдем за допустимое число токенов\n",
        "    for string in strings:\n",
        "        next_article = f'\\n\\nWikipedia article section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
        "        if (num_tokens(message + next_article + question, model=model) > token_budget):\n",
        "            break\n",
        "        else:\n",
        "            message += next_article\n",
        "    return message + question\n",
        "\n",
        "\n",
        "def ask(query: str, df: pd.DataFrame = df, model: str = GPT_MODEL, token_budget: int = 4096 - 500, print_message: bool = False,) -> str:\n",
        "    \"\"\"Отвечает на вопрос, используя GPT и базу знаний.\"\"\"\n",
        "    # Формируем сообщение к chatGPT (функция выше)\n",
        "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
        "    if print_message:\n",
        "        print(message)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": f'You answer questions about the {name_category}.'},\n",
        "        {\"role\": \"user\", \"content\": message},\n",
        "    ]\n",
        "    response = openai.chat.completions.create(model=model, messages=messages,temperature=0)\n",
        "    response_message = response.choices[0].message.content\n",
        "    return response_message"
      ],
      "metadata": {
        "id": "PyMykK1iQ-un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask('When Valeriy Alekseevich was born?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DulexXV4RDOv",
        "outputId": "26a67c9e-46b4-4f57-e6fc-37bd9c9cc01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Valery Alekseyevich Legasov was born on September 1, 1936.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask('Scientific career of Yuri Struchkov')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "GPn75xPkREHW",
        "outputId": "5a1a16d6-eb07-4923-ab26-a2837bbb4c93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yuri Struchkov graduated from the MSU Faculty of Chemistry in 1948. He worked on his Ph.D. thesis under the supervision of professor A.I. Kitaigorodski from 1948 to 1953. In 1950, he joined the staff of the Zelinsky Institute of Organic Chemistry. He defended his PhD thesis on X-Ray structural study in 1953. From 1954 to 1995, he worked at the Nesmeyanov Institute of Organoelement Compounds, initially in the laboratory headed by A.I. Kitaigorodski, then in the diffraction studies group, and later in the new Laboratory of the X-Ray Structure Analysis which he founded and headed. In 1989, Struchkov became the Director of the X-Ray Structural Centre of RAS, based on the INEOS LXRSA. He was elected Corresponding Member of RAS in 1990 and became the vice-president of the International Union of Crystallography from 1993 to 1995.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask('what Russian chemists do you know?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "OntJ8TQ7REjX",
        "outputId": "3c05ee02-f93d-4c45-a0d6-ed7e6112f40e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I know about several Russian chemists, including Pyotr Kapitsa, Salambek Khadzhiyev, Morris Kharasch, Gottlieb Kirchhoff, Ivan Knunyants, Anatoly V. Oleynik, Boris Jacobi, Nikolay Semyonov, Carl Schmidt, Vladimir Shukhov, Mikhail Shultz, Sergei Vasilyevich Lebedev, Mikhail Lomonosov, Aleksandr Loran, Dmitry Chernov, Aleksei Chichibabin, Lev Chugaev, Karl Ernst Claus, Valeriy Chernyshev, Alexander Baykov, Ernest Beaux, Nikolay Beketov, Friedrich Konrad Beilstein, Boris Pavlovich Belousov, Alexander Borodin, Aleksandr Butlerov, Sergey Reformatsky, Sergey Namyotkin, Konstantin Novoselov, Vladimir Markovnikov, Yurii Sh. Matros, Dmitri Mendeleyev, Nikolai Menshutkin, Nikolay Trifonov, Mikhail Tsvet, Andre Geim, Igor Gorynin, Vladimir Ipatieff, Isidore, Paul Walden, Alexander Arbuzov, Nikolay Demyanov, Aleksandr Dianin, Martin Kabachnik, Dmitrii Knorre, George Koval, Ulmas Mirsaidovich Mirsaidov, Valentin Koptyug, Vasily Korshak, Valery Legasov, and Alexander Zaitsev.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Телеграм Бот**"
      ],
      "metadata": {
        "id": "xLw5HvVOp-XB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "JEV29ypq11fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from aiogram import Bot, Dispatcher, types, F\n",
        "from aiogram.filters import Command\n",
        "from aiogram.types import KeyboardButton\n",
        "from aiogram.utils.keyboard import ReplyKeyboardBuilder"
      ],
      "metadata": {
        "id": "i2mtVbonyO3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"API_TOKEN\"] = getpass.getpass(\"Введите телерам API TOKEN:\")\n",
        "\n",
        "bot = Bot(token=os.environ.get(\"API_TOKEN\"))\n",
        "dp = Dispatcher()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50OpM3b-yNIY",
        "outputId": "973ce1d4-1606-4863-c564-0422993d1e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Введите телерам API TOKEN:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Клавиатуры\n",
        "start_keyboard = ReplyKeyboardBuilder()\n",
        "start_keyboard.add(KeyboardButton(text='Начать разговор'),\n",
        "                   KeyboardButton(text='Информация о базе знаний'),\n",
        "                   )\n",
        "start_keyboard = start_keyboard.adjust(1, 1).as_markup(resize_keyboard=True, )"
      ],
      "metadata": {
        "id": "aMadO8nM03aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# хэндлеры\n",
        "@dp.message(Command(\"start\"))\n",
        "async def cmd_start(message: types.Message):\n",
        "    await message.answer(\"Добро пожаловать в GPT бот!\", reply_markup=start_keyboard)\n",
        "\n",
        "\n",
        "@dp.message(Command(\"help\"))\n",
        "@dp.message(F.text == \"Информация о базе знаний\")\n",
        "async def cmd_help(message: types.Message):\n",
        "    await message.answer(f\"Тематика разговора: {name_category}\")\n",
        "    await message.answer(f\"Число записей в базе знаний: {len(wikipedia_strings)}\")\n",
        "    await message.answer(\"Пример запроса к базе: When Valeriy Alekseevich was born?, Evgenii Przhevalsky Biography, who was born on May 20, 1939?, Russian chemists have listed\")\n",
        "\n",
        "\n",
        "@dp.message(F.text == \"Начать разговор\")\n",
        "async def cmd_start_text(message: types.Message):\n",
        "    await message.answer(\"Введите ваш вопрос то теме:\")\n",
        "\n",
        "\n",
        "@dp.message()\n",
        "async def cmd_text(message: types.Message):\n",
        "    reply = ask(message.text)\n",
        "    await message.answer(f'Ответ: {reply}')\n"
      ],
      "metadata": {
        "id": "cl3sCxAYy_J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Запуск бота\n",
        "async def main():\n",
        "    await dp.start_polling(bot)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "-0E3vwdTqBrY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}